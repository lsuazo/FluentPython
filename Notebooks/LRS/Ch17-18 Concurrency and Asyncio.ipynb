{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH 17 and 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC<a id='toc'></a>\n",
    "* [Ch17 Notes](#ch17_notes)\n",
    "* [Ch18 Notes](#ch18_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CH17 Notes <a id='ch17_notes'></a>\n",
    "[toc](#toc)\n",
    "### Concurrency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the `requests` library by Keneth Reitz is more powerful and easier to use than urllib.request module from python 3 std library\n",
    "    * is considered a model Pythonic API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading with concurrent.futures\n",
    "* main features are `ThreadPoolExecutor` and `ProcessPoolExecutor` classes\n",
    "    - implement interfaces that allow you to submit callables for execution in different threads or processes respectively\n",
    "    - they manage an internal pool of threads or processes and a queue of tasks to be exectuted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example code:\n",
    "```\n",
    "def download_many(cc_list):\n",
    "    workers = min(MAX_WORKERS, len(cc_list))\n",
    "    with futures.ThreadPoolExecutor(workers) as executor:\n",
    "        res = executor.map(download_one, sorted(cc_list))\n",
    "        \n",
    "       return len(list(res))\n",
    "```\n",
    "\n",
    "* `executor.__exit__` method will call `executor.shutdown(wait=True)`, which will block untill all threads are done.\n",
    "* map function returns a generator that can be iterated over to retrieve the value returned by each function.\n",
    "    - if any function raised an exception, the exception would be raised when calling next on that generator\n",
    "* very often, body of the loop is refactored into a separate function when calling either sequentially or concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Futures?\n",
    "* as of python 3.4 two Futures classes: `concurrent.futures.Futures` and `asyncio.Future`\n",
    "       - an instance of either class represents a deferred computation that may or may not have completed.\n",
    "       - their state of completion can be queried, and their results (or exceptions) can be retrieved when avaialble.\n",
    "* You and I should not create them! They should be instantiated exclusively by concurrency framework.\n",
    "    - they are instantiated when some work is scheduled - this typically returns a future.\n",
    "* both have a .done() method to check if it is done - but usually you use the `.add_done_callback()` and provide a callable instead.\n",
    "* there is also a `.result()` method, which if done, returns result, or re-reaises any exceptions. \n",
    "    - if future is not done, result behavior is very different for the two future classes: concurrency.future blocks and has optional timeout, asyncio does not support timeout, and preferred is to use yield from - which doesn't work with concurrency.\n",
    "* Executor.map returns an iterator in which `__next__` calls the `result` method of each future.\n",
    "* concurrent.futures has an .as_completed() function that takes an iterable of futures and returns an iterator that yiedls futures as they are done.\n",
    "    - they are yielded in whatever order they finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking I/O and the GIL\n",
    "* The CPython interpreter is not thread-safe internally, so it has a Global Interpreted Lock (GIL), which allows only one thread at a time to executed python bytecodes. That's why a single python processs usually cannot use multiple CPU cores at the same time.\n",
    "    - when we write python code, we have no control over the GIL, but a built-in function, or an extension written in C can release the GIL. (This complciates the code of the library considerably, so most authors don't do it.)\n",
    "* however all standard libray functions that perform blockin I/O release the GIL when waiting for a result from the OS. So while one Python thread is waiting for a response from the network (or other?), the blocked I/O function releases the GIL so another thread can run.\n",
    "    - time.sleep() function also releases the GIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProcessPoolExecutor\n",
    "* truly parallel computation - bypasses GIL and leverages all available CPUs\n",
    "* won't help much if the work is I/O bound, where much of the time is passed waiting - threads are better for this\n",
    "    - not limited by number od CPUs\n",
    "* If you are using CPU-intensive work in python, you should try PyPy - it is much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Executor.map is easy to use, but it returns results in the same order as the calls are started; so your code might block waiting for a result, while other results further down the call line have already completed.\n",
    "* to avoid this use `Executor.submit` and `futures.as_completed`\n",
    "    - this is also more flexible because you can use different callables and arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads with Progress Display and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:10<00:00, 95.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* can handle some error in the callable; can handle other errors in the function that creates the executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usually when using `futures.as_completed` it is useful to build a dcit mapping each future to other data that may be useful when future is completed (in the flag example, the future is mapped to the country code)\n",
    "    - makes it easy to do a follow up process despite the fact that they are out of order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threading and multiprocessing alternatives\n",
    "* originally had `thread` module; since python 3 it was deprecated in favor of a higher level `threading` module\n",
    "    - original renamed to `_thread` to make obvious it is just low-level implementation detail and shouldn't be used\n",
    "* futures.ThreadPoolExecutor is a high level access for this, but if it doesn't meet your needs, you might need to build your own solution from `threading`\n",
    "* For CPU_bound work, sidestep GIL with `futures.ProcessPoolExecutor`. But if your case is too complex, you need more advance tools like `multiprocessing`\n",
    "    - `multiprocessing` also offers facilities to solve the biggest challenge faced by collaborating processes: *(how to pass data around*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best advice: **Don't try to manage threads and locks** - let system programmers that know what they are doing do that; you should just use what they have built.\n",
    "\n",
    "- of course, these are generally designed for simple, so called **embarassingly parallel** jobs in mind, but that is typically what you need.\n",
    "- as opposed to operating systems or database servers\n",
    "\n",
    "Even for non embarrisingly simple, threads and locks are not the answer - use higher level abstractions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CH18 Notes <a id='ch18_notes'></a>\n",
    "[toc](#toc)\n",
    "### Concurrency with asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Concurrency** is about dealing with lots of things at once\n",
    "- **Parallelism** is about doing lots of things at once\n",
    "- Not the same, but related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A moder laptop has 4 CPU cores but is routinely running more than 100 processes at any given time under normal, causal use. So in practice, most processsing happens concurrently, not in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `asyncio` is a package that implements concurrency with coroutines driven by an event loop.\n",
    "    - one of the largest and most ambitious libraries ever added to python\n",
    "    - developed by the benevolent dictator himself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blast off!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    mess = str(i)\n",
    "    print(mess,end='')\n",
    "    time.sleep(1)\n",
    "    print('\\x08'*len(mess), end='')\n",
    "print('Blast off!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `other_thread = threading.Thread(target=..., args=(...))` can be used to  create new threads\n",
    "    - other_thread.start() kicks it off\n",
    "    - typically comminunicate with it via some object which it has a reference for, and you can manipulate object in main thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Asyncio is stricter about corountines: must use the **yield from** and not the **yield** in the body\n",
    "* shoud be driven by caller using *yield from*, or by `asyncio.async()` function\n",
    "* should be decorated with @asyncio.coroutine [not mandatory but highly advisable]\n",
    "* you can communicate with ascyncio corroutines directly - `spinner.cancel()`, and don't need to communicate via shared reference to signal objec as with threading\n",
    "* use `asyncio.sleep()` instead of `time.sleep()` to sleep without blocking the event loop\n",
    "* ayncio uses *task* abstraction - they drive coroutines\n",
    "    - similar to thread invoking callables\n",
    "    - you don't instantiate them yourself, but are returned from asyncio.async(), etc.\n",
    "* with threads, you must be careful to hold locks, etc. to protect from interruption - with coroutins, everything is protected from interaction by default.\n",
    "    * you must explicitly yield to let the rest of the program run\n",
    "    * only one corroutine runs at a time - yielding gives control back to scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trick suggested by Guido van Rossum: \"squint and pretend the yield from keywords are not there\" - this makes it easier to follow asyncio code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary: as we use `asyncio`, our asynchronous code consists of coroutines that are delegating generators friven by `asyncio` itself and that ultimately delegate to `asyncio` library coroutines - possibly by way of some third-party library such as `aiohttp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If an L1 cache read took 1 sec, a network read would take something like 2.5 years. Can't halt application for that\n",
    "* solutions:\n",
    "    - run each blocking operation in a separate thread\n",
    "    - turn each blocking operation into a nonblocking asynchronous call\n",
    "* Threads work fine, but the memory overhead ofr each OS thread (the kind that python uses) is on the order of MB. Can't affor id using thousands.\n",
    "* Callbacks are the traditional way to implement asynchronous calls with low memory ovehread [ME: are these not still using a bunch of OS threads?]\n",
    "    - they are a low-level concept, similart to the oldest and most primitive concurrency mechanism of all: hardware interrupts\n",
    "* of course, we can only make callbacks work becaue the event loop underlying our asynchronous applications can rely on infrastructures that uses interrups, theads, polling, background processes, etc. to ensure that multiple concurrent requests make progress and eventually get done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* here use **semaphore** as throltling device\n",
    "* a **semaphore** is an object that holds and internal counter that is decremented whenever we call the `.acquire()` coroutine method on it, and incremented when we call the `.release()` coroutine methods.\n",
    "    - initial value of the counter is set when the Semaphore is instantiated\n",
    "    - in the book he uses the semaphore as a context manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* local filesystem access is blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* corroutines are a big improvement over nested callbacks for trying to synchronize asychronous coroutine, by replacing the call back with yield from "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
